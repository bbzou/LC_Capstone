{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# from HelperFunctions import minibatch \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from HelperFunctions import minibatch, dummify_columns, undummify, feature_standardize, label_encode_column, columns_of_type\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# For Bayesian Optimizer\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, AdaBoostRegressor\n",
    "randomForest = RandomForestRegressor()\n",
    "gbm = GradientBoostingRegressor()\n",
    "abr = AdaBoostRegressor()\n",
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "from lightgbm import LGBMRegressor\n",
    "lgb = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_prob(unsampled_df,sampled_df,sampled_prob): # sampled_df, sampled_prob can also be test_df, test_prob\n",
    "    # turning the predicted probability into a dataframe with column name default_prob\n",
    "    sample_prob_df=pd.DataFrame(sampled_prob[:,0],columns=['sampled_prob'])\n",
    "    # find actual default rate for each class\n",
    "    grade_rate=unsampled_df.groupby('grade')['loan_status'].apply(lambda x:(x=='Default').sum()/x.count())\n",
    "    grade_rate_dict=grade_rate.to_dict()\n",
    "    # mapping the unsampled_df default rates to the test_df=sampled_df\n",
    "    # and then getting the array of default_rates in the test_df\n",
    "    sampled_df['default_rate']=sampled_df['grade'].map(grade_rate_dict)\n",
    "    sampled_df.reset_index(drop=True, inplace=True)\n",
    "    sample_prob_df.reset_index(drop=True, inplace=True)\n",
    "    pre_adjust_df=pd.concat([sampled_df,sample_prob_df],axis=1)\n",
    "    # Adjusting the default_probability to the true probability (accounting for down/up sampling)    \n",
    "    sampled_frac=0.5\n",
    "    real_prob=[]\n",
    "    for row in pre_adjust_df.loc[:,['default_rate','sampled_prob']].iterrows():\n",
    "        beta=sampled_frac/(1-row[1]['default_rate'])\n",
    "        real_prob.append(beta*row[1]['sampled_prob']/((beta-1)*row[1]['sampled_prob']+1))\n",
    "        #     prob=1/(1+(1/original_fraction-1)/(1/sampled_fraction-1)*(1/sampled_prob-1))\n",
    "    a=pd.DataFrame(real_prob,columns=['actual_prob'])\n",
    "    b=pd.DataFrame(sampled_prob[:,0],columns=['downsampled_prob'])\n",
    "    return pd.concat([a,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('down_sampled_df_v2.csv')\n",
    "pre_df=pd.read_csv('pre_downsample_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Grade Weights HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_weight_dict={'A':1,\n",
    "                  'B':2,\n",
    "                  'C':3,\n",
    "                  'D':4,\n",
    "                  'E':5,\n",
    "                  'F':6,\n",
    "                  'G':7}\n",
    "\n",
    "df['weight']=df['grade'].map(grade_weight_dict)\n",
    "grade_weight_array=df['weight'].values\n",
    "index = df.index\n",
    "grade_weight_frame = pd.DataFrame(grade_weight_array, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping features not needed for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list=['sub_grade','issue_d','zip_code','RANDOM','id','weight']\n",
    "df.drop(drop_list,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=df.drop(['loan_status','return_rate'],axis=1)\n",
    "y_train=df.loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list=columns_of_type(x_train,'string')\n",
    "cont_list=columns_of_type(x_train,'number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Encode ALL Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_encoded_df=label_encode_column(x_train,cat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize ALL Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df=label_encoded_df.apply(lambda x:feature_standardize(x,scaleType='standardize'),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM SCORING FUNCTION: WEIGHTED ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "def grade_weighted_accuracy(y_true,y_pred,sample_weight):\n",
    "\n",
    "    weighted_acc=accuracy_score(y_true,y_pred,\n",
    "                                sample_weight=sample_weight.loc[y_true.index.values].values.reshape(-1),\n",
    "                                normalize=True)\n",
    "    return weighted_acc    \n",
    "\n",
    "score_params = {\"sample_weight\": grade_weight_frame}\n",
    "\n",
    "grade_weighted_scorer = make_scorer(score_func=grade_weighted_accuracy,\n",
    "                                    greater_is_better=True,\n",
    "                                    needs_proba=False,\n",
    "                                    needs_threshold=False,\n",
    "                                    **score_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_para_forest = {\n",
    "#     'n_estimators': range(20, 80, 2),\n",
    "#     'max_depth':[2,3],\n",
    "#     'max_features':[1, 2],\n",
    "#     'min_samples_leaf':[1,2],\n",
    "#     'min_samples_split':[2,3]\n",
    "# }\n",
    "\n",
    "grid_para_forest = {\n",
    "    'min_samples_leaf':[1,2],\n",
    "    'min_samples_split':[2,3]\n",
    "}\n",
    "\n",
    "grid_search_forest = GridSearchCV(estimator=RandomForestClassifier(n_estimators=10,max_depth=2,max_features=2),\\\n",
    "                                  param_grid=grid_para_forest,\\\n",
    "                                  n_jobs=-1,\\\n",
    "                                  scoring=grade_weighted_scorer,cv=5,\\\n",
    "                                  return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "best score: 0.5963810813453226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[19801,  6269],\n",
       "       [13994, 11808]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_forest.fit(final_train_df,y_train)\n",
    "print('best parameters:', grid_search_forest.best_params_)\n",
    "print('best score:', grid_search_forest.best_score_)\n",
    "confusion_matrix(y_train, grid_search_forest.predict(final_train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Probabilities to account for downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_prob</th>\n",
       "      <th>downsampled_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332095</td>\n",
       "      <td>0.483204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.273819</td>\n",
       "      <td>0.414882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.317502</td>\n",
       "      <td>0.466610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.303558</td>\n",
       "      <td>0.450439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277691</td>\n",
       "      <td>0.419597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.363830</td>\n",
       "      <td>0.518176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.349545</td>\n",
       "      <td>0.502619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.349853</td>\n",
       "      <td>0.502958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.365103</td>\n",
       "      <td>0.519548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.272661</td>\n",
       "      <td>0.413468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.367434</td>\n",
       "      <td>0.522054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.367843</td>\n",
       "      <td>0.522493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.354500</td>\n",
       "      <td>0.508049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.349801</td>\n",
       "      <td>0.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.344123</td>\n",
       "      <td>0.496635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.378964</td>\n",
       "      <td>0.534337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.346963</td>\n",
       "      <td>0.499775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.387662</td>\n",
       "      <td>0.543481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.299920</td>\n",
       "      <td>0.446168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.322375</td>\n",
       "      <td>0.472188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.365877</td>\n",
       "      <td>0.520381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.365446</td>\n",
       "      <td>0.519917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.299004</td>\n",
       "      <td>0.445089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.323038</td>\n",
       "      <td>0.472944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.354107</td>\n",
       "      <td>0.507619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.268465</td>\n",
       "      <td>0.408321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.370979</td>\n",
       "      <td>0.525850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.297304</td>\n",
       "      <td>0.443084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.341323</td>\n",
       "      <td>0.493528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.323022</td>\n",
       "      <td>0.472926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51842</th>\n",
       "      <td>0.424398</td>\n",
       "      <td>0.428072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51843</th>\n",
       "      <td>0.528186</td>\n",
       "      <td>0.531929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51844</th>\n",
       "      <td>0.515914</td>\n",
       "      <td>0.519666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51845</th>\n",
       "      <td>0.533749</td>\n",
       "      <td>0.537486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51846</th>\n",
       "      <td>0.513183</td>\n",
       "      <td>0.516936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51847</th>\n",
       "      <td>0.521475</td>\n",
       "      <td>0.525223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51848</th>\n",
       "      <td>0.481899</td>\n",
       "      <td>0.485651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51849</th>\n",
       "      <td>0.520417</td>\n",
       "      <td>0.524166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51850</th>\n",
       "      <td>0.505159</td>\n",
       "      <td>0.508915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51851</th>\n",
       "      <td>0.519798</td>\n",
       "      <td>0.523547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51852</th>\n",
       "      <td>0.505306</td>\n",
       "      <td>0.509062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51853</th>\n",
       "      <td>0.511363</td>\n",
       "      <td>0.515116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51854</th>\n",
       "      <td>0.478253</td>\n",
       "      <td>0.482003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51855</th>\n",
       "      <td>0.525254</td>\n",
       "      <td>0.528999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51856</th>\n",
       "      <td>0.523047</td>\n",
       "      <td>0.526794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51857</th>\n",
       "      <td>0.525359</td>\n",
       "      <td>0.529104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51858</th>\n",
       "      <td>0.513655</td>\n",
       "      <td>0.517408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51859</th>\n",
       "      <td>0.509152</td>\n",
       "      <td>0.512907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51860</th>\n",
       "      <td>0.491728</td>\n",
       "      <td>0.495484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51861</th>\n",
       "      <td>0.427801</td>\n",
       "      <td>0.431483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51862</th>\n",
       "      <td>0.474419</td>\n",
       "      <td>0.478167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51863</th>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.498706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51864</th>\n",
       "      <td>0.515894</td>\n",
       "      <td>0.519646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51865</th>\n",
       "      <td>0.535352</td>\n",
       "      <td>0.539087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51866</th>\n",
       "      <td>0.525254</td>\n",
       "      <td>0.528999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51867</th>\n",
       "      <td>0.524551</td>\n",
       "      <td>0.528297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51868</th>\n",
       "      <td>0.521998</td>\n",
       "      <td>0.525745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51869</th>\n",
       "      <td>0.540196</td>\n",
       "      <td>0.543926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51870</th>\n",
       "      <td>0.543258</td>\n",
       "      <td>0.546983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51871</th>\n",
       "      <td>0.479453</td>\n",
       "      <td>0.483204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51872 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual_prob  downsampled_prob\n",
       "0         0.332095          0.483204\n",
       "1         0.273819          0.414882\n",
       "2         0.317502          0.466610\n",
       "3         0.303558          0.450439\n",
       "4         0.277691          0.419597\n",
       "5         0.363830          0.518176\n",
       "6         0.349545          0.502619\n",
       "7         0.349853          0.502958\n",
       "8         0.365103          0.519548\n",
       "9         0.272661          0.413468\n",
       "10        0.367434          0.522054\n",
       "11        0.367843          0.522493\n",
       "12        0.354500          0.508049\n",
       "13        0.349801          0.502900\n",
       "14        0.344123          0.496635\n",
       "15        0.378964          0.534337\n",
       "16        0.346963          0.499775\n",
       "17        0.387662          0.543481\n",
       "18        0.299920          0.446168\n",
       "19        0.322375          0.472188\n",
       "20        0.365877          0.520381\n",
       "21        0.365446          0.519917\n",
       "22        0.299004          0.445089\n",
       "23        0.323038          0.472944\n",
       "24        0.354107          0.507619\n",
       "25        0.268465          0.408321\n",
       "26        0.370979          0.525850\n",
       "27        0.297304          0.443084\n",
       "28        0.341323          0.493528\n",
       "29        0.323022          0.472926\n",
       "...            ...               ...\n",
       "51842     0.424398          0.428072\n",
       "51843     0.528186          0.531929\n",
       "51844     0.515914          0.519666\n",
       "51845     0.533749          0.537486\n",
       "51846     0.513183          0.516936\n",
       "51847     0.521475          0.525223\n",
       "51848     0.481899          0.485651\n",
       "51849     0.520417          0.524166\n",
       "51850     0.505159          0.508915\n",
       "51851     0.519798          0.523547\n",
       "51852     0.505306          0.509062\n",
       "51853     0.511363          0.515116\n",
       "51854     0.478253          0.482003\n",
       "51855     0.525254          0.528999\n",
       "51856     0.523047          0.526794\n",
       "51857     0.525359          0.529104\n",
       "51858     0.513655          0.517408\n",
       "51859     0.509152          0.512907\n",
       "51860     0.491728          0.495484\n",
       "51861     0.427801          0.431483\n",
       "51862     0.474419          0.478167\n",
       "51863     0.494949          0.498706\n",
       "51864     0.515894          0.519646\n",
       "51865     0.535352          0.539087\n",
       "51866     0.525254          0.528999\n",
       "51867     0.524551          0.528297\n",
       "51868     0.521998          0.525745\n",
       "51869     0.540196          0.543926\n",
       "51870     0.543258          0.546983\n",
       "51871     0.479453          0.483204\n",
       "\n",
       "[51872 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_prob=grid_search_forest.predict_proba(final_train_df)\n",
    "adjust_prob(pre_df,df,sampled_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing without weighted accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search_forest = GridSearchCV(estimator=RandomForestClassifier(n_estimators=10,max_depth=2,max_features=2),\\\n",
    "#                                   param_grid=grid_para_forest,\\\n",
    "#                                   n_jobs=-1,\\\n",
    "#                                   cv=5,\\\n",
    "#                                   return_train_score=False)\n",
    "# grid_search_forest.fit(final_train_df,y_train)\n",
    "# print('best parameters:', grid_search_forest.best_params_)\n",
    "# print('best score:', grid_search_forest.best_score_)\n",
    "# confusion_matrix(y_train, grid_search_forest.predict(final_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_prob=grid_search_forest.predict_proba(final_train_df)\n",
    "# adjust_prob(pre_df,df,sampled_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
